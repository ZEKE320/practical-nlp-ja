{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "04_Tokenization_Stemming_lemmatization_stopword_postagging.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QnXzcZvdc-r6"
   },
   "source": [
    "# spaCyとNLTKを用いた前処理\n",
    "\n",
    "このノートブックでは、[spaCy](https://spacy.io/)と[nltk](https://www.nltk.org/)を用いて、単語分割、ステミング、見出し語化、品詞タグ付けを行います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eOEyaHAzyDhu"
   },
   "source": [
    "## コーパス\n",
    "\n",
    "今回は、以下のコーパスを例に取り組みます。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "R3xEmJpRc5r8"
   },
   "source": [
    "corpus_original = \"Need to finalize the demo corpus which will be used for this notebook and it should be done soon !!. It should be done by the ending of this month. But will it? This notebook has been run 4 times !!\"\n",
    "corpus = \"Need to finalize the demo corpus which will be used for this notebook & should be done soon !!. It should be done by the ending of this month. But will it? This notebook has been run 4 times !!\""
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YDr8nkDNySV9"
   },
   "source": [
    "## 基本的な前処理\n",
    "\n",
    "基本的な前処理として以下を行います。\n",
    "\n",
    "- 小文字化\n",
    "- 数字の除去\n",
    "- 句読点の除去\n",
    "- 末尾の空白の除去"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KHh_33IopPTf",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "f0b29530-7a1e-4730-b7f0-057108bd28fa"
   },
   "source": [
    "corpus = corpus.lower()\n",
    "print(corpus)"
   ],
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "need to finalize the demo corpus which will be used for this notebook & should be done soon !!. it should be done by the ending of this month. but will it? this notebook has been run 4 times !!\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3yaGf8RiqgBM",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "289532bb-1fea-4dcc-a128-82b53bf07ab7"
   },
   "source": [
    "import re\n",
    "corpus = re.sub(r'\\d+','', corpus)\n",
    "print(corpus)"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "need to finalize the demo corpus which will be used for this notebook & should be done soon !!. it should be done by the ending of this month. but will it? this notebook has been run  times !!\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "v5Q--GItqzfu",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "b4416891-3dde-4984-de63-9406a7047fd5"
   },
   "source": [
    "import string\n",
    "corpus = corpus.translate(str.maketrans('', '', string.punctuation))\n",
    "print(corpus)"
   ],
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "need to finalize the demo corpus which will be used for this notebook  should be done soon  it should be done by the ending of this month but will it this notebook has been run  times \n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zmANqee9rK4N",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "outputId": "2d9c7984-2964-4c0e-b231-44ece17f5b82"
   },
   "source": [
    "corpus = ' '.join([token for token in corpus.split()])\n",
    "corpus"
   ],
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'need to finalize the demo corpus which will be used for this notebook should be done soon it should be done by the ending of this month but will it this notebook has been run times'"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 5
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S79Eh-mnys0y"
   },
   "source": [
    "## spaCyとNLTKを用いた前処理\n",
    "### パッケージのインストール\n",
    "\n",
    "spaCyをインストールし、英語のモデルをダウンロードします。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KMuHZTpy9X_u",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "9b77e4e6-a245-409f-a68a-de8f1382eb92"
   },
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm"
   ],
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
      "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.0.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.41.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.5.30)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (4.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.4.1)\n",
      "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.7/dist-packages (2.2.5)\n",
      "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
      "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (57.0.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2021.5.30)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.1)\n",
      "\u001B[38;5;2m✔ Download and installation successful\u001B[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nfJx3MnVj_ph"
   },
   "source": [
    "### テキストの単語分割\n",
    "\n",
    "では、NLTKとspaCyを用いて、テキストを単語に分割してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OUz580k2sMqf",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "dd24a678-f858-4395-891d-2a2469b44778"
   },
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stop_words_nltk = set(stopwords.words('english'))\n",
    "tokenized_corpus_nltk = word_tokenize(corpus)\n",
    "print('Tokenized corpus:', tokenized_corpus_nltk)\n",
    "\n",
    "tokenized_corpus_without_stopwords = [i for i in tokenized_corpus_nltk if not i in stop_words_nltk]\n",
    "print('Tokenized corpus without stopwords:', tokenized_corpus_without_stopwords)"
   ],
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "Tokenized corpus: ['need', 'to', 'finalize', 'the', 'demo', 'corpus', 'which', 'will', 'be', 'used', 'for', 'this', 'notebook', 'should', 'be', 'done', 'soon', 'it', 'should', 'be', 'done', 'by', 'the', 'ending', 'of', 'this', 'month', 'but', 'will', 'it', 'this', 'notebook', 'has', 'been', 'run', 'times']\n",
      "Tokenized corpus without stopwords: ['need', 'finalize', 'demo', 'corpus', 'used', 'notebook', 'done', 'soon', 'done', 'ending', 'month', 'notebook', 'run', 'times']\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AlpE77iHzQsv",
    "outputId": "bbeff6f1-54d7-440a-ea2b-1e41f23c6e9a"
   },
   "source": [
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "spacy_model = spacy.load('en_core_web_sm')\n",
    "\n",
    "stopwords_spacy = spacy_model.Defaults.stop_words\n",
    "tokenized_corpus_spacy = word_tokenize(corpus)\n",
    "print('Tokenized Corpus:', tokenized_corpus_spacy)\n",
    "\n",
    "tokens_without_sw= [word for word in tokenized_corpus_spacy if not word in stopwords_spacy]\n",
    "print('Tokenized corpus without stopwords', tokens_without_sw)"
   ],
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Tokenized Corpus: ['need', 'to', 'finalize', 'the', 'demo', 'corpus', 'which', 'will', 'be', 'used', 'for', 'this', 'notebook', 'should', 'be', 'done', 'soon', 'it', 'should', 'be', 'done', 'by', 'the', 'ending', 'of', 'this', 'month', 'but', 'will', 'it', 'this', 'notebook', 'has', 'been', 'run', 'times']\n",
      "Tokenized corpus without stopwords ['need', 'finalize', 'demo', 'corpus', 'notebook', 'soon', 'ending', 'month', 'notebook', 'run', 'times']\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uyhXyu8xzzBK",
    "outputId": "7bae0cd5-9cdc-444b-d8ac-69c8cafcaf2d"
   },
   "source": [
    "print('Difference between NLTK and spaCy output:\\n',\n",
    "      set(tokenized_corpus_without_stopwords) - set(tokens_without_sw))"
   ],
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Difference between NLTK and spaCy output:\n",
      " {'used', 'done'}\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tGcwD1JlkEao"
   },
   "source": [
    "### ステミング"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ibEpzcv0sdW8",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "9eae87f9-6925-43eb-f7ce-5e93b7b6dd70"
   },
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "stemmer= PorterStemmer()\n",
    "\n",
    "print('Before Stemming:')\n",
    "print(corpus)\n",
    "\n",
    "print('After Stemming:')\n",
    "for word in tokenized_corpus_nltk:\n",
    "    print(stemmer.stem(word), end=' ')"
   ],
   "execution_count": 12,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Before Stemming:\n",
      "need to finalize the demo corpus which will be used for this notebook should be done soon it should be done by the ending of this month but will it this notebook has been run times\n",
      "After Stemming:\n",
      "need to final the demo corpu which will be use for thi notebook should be done soon it should be done by the end of thi month but will it thi notebook ha been run time "
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Wy6cwvYkJeR"
   },
   "source": [
    "### 見出し語化"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "27KvL4ZE-fqJ",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "5e45f4f3-bd88-42c0-bde2-a5192fe71ea4"
   },
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('wordnet')\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "for word in tokenized_corpus_nltk:\n",
    "    print(lemmatizer.lemmatize(word), end=' ')"
   ],
   "execution_count": 13,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
      "need to finalize the demo corpus which will be used for this notebook should be done soon it should be done by the ending of this month but will it this notebook ha been run time "
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h8uCGA8ukMfQ"
   },
   "source": [
    "### 品詞タグ付け"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LZoVopT40gmd",
    "outputId": "0b03141d-9e72-487a-be4e-060b98d5403f"
   },
   "source": [
    "from pprint import pprint\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "print('POS Tagging using NLTK:')\n",
    "pprint(nltk.pos_tag(word_tokenize(corpus_original)))"
   ],
   "execution_count": 15,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "POS Tagging using NLTK:\n",
      "[('Need', 'NN'),\n",
      " ('to', 'TO'),\n",
      " ('finalize', 'VB'),\n",
      " ('the', 'DT'),\n",
      " ('demo', 'NN'),\n",
      " ('corpus', 'NN'),\n",
      " ('which', 'WDT'),\n",
      " ('will', 'MD'),\n",
      " ('be', 'VB'),\n",
      " ('used', 'VBN'),\n",
      " ('for', 'IN'),\n",
      " ('this', 'DT'),\n",
      " ('notebook', 'NN'),\n",
      " ('and', 'CC'),\n",
      " ('it', 'PRP'),\n",
      " ('should', 'MD'),\n",
      " ('be', 'VB'),\n",
      " ('done', 'VBN'),\n",
      " ('soon', 'RB'),\n",
      " ('!', '.'),\n",
      " ('!', '.'),\n",
      " ('.', '.'),\n",
      " ('It', 'PRP'),\n",
      " ('should', 'MD'),\n",
      " ('be', 'VB'),\n",
      " ('done', 'VBN'),\n",
      " ('by', 'IN'),\n",
      " ('the', 'DT'),\n",
      " ('ending', 'VBG'),\n",
      " ('of', 'IN'),\n",
      " ('this', 'DT'),\n",
      " ('month', 'NN'),\n",
      " ('.', '.'),\n",
      " ('But', 'CC'),\n",
      " ('will', 'MD'),\n",
      " ('it', 'PRP'),\n",
      " ('?', '.'),\n",
      " ('This', 'DT'),\n",
      " ('notebook', 'NN'),\n",
      " ('has', 'VBZ'),\n",
      " ('been', 'VBN'),\n",
      " ('run', 'VBN'),\n",
      " ('4', 'CD'),\n",
      " ('times', 'NNS'),\n",
      " ('!', '.'),\n",
      " ('!', '.')]\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kZqBxLDz-6cu",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "fabf40a4-f371-48c9-e831-586857b48f3a"
   },
   "source": [
    "doc = spacy_model(corpus_original)\n",
    "\n",
    "print('POS Tagging using spacy:')\n",
    "for token in doc:\n",
    "  print(f'{token}\\t{token.pos_}')"
   ],
   "execution_count": 18,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "POS Tagging using spacy:\n",
      "Need\tVERB\n",
      "to\tPART\n",
      "finalize\tVERB\n",
      "the\tDET\n",
      "demo\tNOUN\n",
      "corpus\tNOUN\n",
      "which\tDET\n",
      "will\tVERB\n",
      "be\tAUX\n",
      "used\tVERB\n",
      "for\tADP\n",
      "this\tDET\n",
      "notebook\tNOUN\n",
      "and\tCCONJ\n",
      "it\tPRON\n",
      "should\tVERB\n",
      "be\tAUX\n",
      "done\tVERB\n",
      "soon\tADV\n",
      "!\tPUNCT\n",
      "!\tPUNCT\n",
      ".\tPUNCT\n",
      "It\tPRON\n",
      "should\tVERB\n",
      "be\tAUX\n",
      "done\tVERB\n",
      "by\tADP\n",
      "the\tDET\n",
      "ending\tNOUN\n",
      "of\tADP\n",
      "this\tDET\n",
      "month\tNOUN\n",
      ".\tPUNCT\n",
      "But\tCCONJ\n",
      "will\tVERB\n",
      "it\tPRON\n",
      "?\tPUNCT\n",
      "This\tDET\n",
      "notebook\tNOUN\n",
      "has\tAUX\n",
      "been\tAUX\n",
      "run\tVERB\n",
      "4\tNUM\n",
      "times\tNOUN\n",
      "!\tPUNCT\n",
      "!\tPUNCT\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zWdmz6lFkpEI"
   },
   "source": [
    "NLTKとspaCy以外にも、基本的な前処理をできるライブラリは他にもあります。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KVZDZtvl037C"
   },
   "source": [
    ""
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}