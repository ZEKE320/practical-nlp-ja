{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02_Doc2Vec_Example.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCgVnQopb6TI"
      },
      "source": [
        "# Doc2Vecを用いたテキスト分類\n",
        "\n",
        "このノートブックでは、Doc2Vecを用いたテキスト分類の方法を説明します。データセットとしては、Kaggleで公開されている「[Sentiment and Emotion in Text](https://www.kaggle.com/c/sa-emotions/data)」を利用します。\n",
        "\n",
        "このデータセットは、一般的なタスクであるセンチメント分析の1つで、テキストの感情的内容（喜び、悲しみ、怒りなど）のラベルを含んでいます。13個のラベルに数百から数千の例が含まれています。このデータのサブセットは、MicrosoftのCortana Intelligence Galleryにアップロードした実験で使用されています．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "br1aiJ6286io"
      },
      "source": [
        "## 準備\n",
        "### パッケージのインポート"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSB6W1seb6TJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "577a43a3-0101-4578-de64-a044c97851ca"
      },
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHtZF4Xy_N_O"
      },
      "source": [
        "### データセットのダウンロード"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fnv168QRJjxi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8aa0ed79-8879-4647-f29c-d66266b1d8ca"
      },
      "source": [
        "!wget -P DATAPATH https://raw.githubusercontent.com/practical-nlp/practical-nlp/master/Ch4/Data/Sentiment%20and%20Emotion%20in%20Text/train_data.csv\n",
        "!wget -P DATAPATH https://raw.githubusercontent.com/practical-nlp/practical-nlp/master/Ch4/Data/Sentiment%20and%20Emotion%20in%20Text/test_data.csv\n",
        "!ls -lah DATAPATH"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-09-07 11:20:36--  https://raw.githubusercontent.com/practical-nlp/practical-nlp/master/Ch4/Data/Sentiment%20and%20Emotion%20in%20Text/train_data.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2479133 (2.4M) [text/plain]\n",
            "Saving to: ‘DATAPATH/train_data.csv’\n",
            "\n",
            "train_data.csv      100%[===================>]   2.36M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2021-09-07 11:20:36 (27.8 MB/s) - ‘DATAPATH/train_data.csv’ saved [2479133/2479133]\n",
            "\n",
            "--2021-09-07 11:20:36--  https://raw.githubusercontent.com/practical-nlp/practical-nlp/master/Ch4/Data/Sentiment%20and%20Emotion%20in%20Text/test_data.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 783640 (765K) [text/plain]\n",
            "Saving to: ‘DATAPATH/test_data.csv’\n",
            "\n",
            "test_data.csv       100%[===================>] 765.27K  --.-KB/s    in 0.07s   \n",
            "\n",
            "2021-09-07 11:20:36 (10.5 MB/s) - ‘DATAPATH/test_data.csv’ saved [783640/783640]\n",
            "\n",
            "total 3.2M\n",
            "drwxr-xr-x 2 root root 4.0K Sep  7 11:20 .\n",
            "drwxr-xr-x 1 root root 4.0K Sep  7 11:20 ..\n",
            "-rw-r--r-- 1 root root 766K Sep  7 11:20 test_data.csv\n",
            "-rw-r--r-- 1 root root 2.4M Sep  7 11:20 train_data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vjz6sbYt_Rp4"
      },
      "source": [
        "### データセットの読み込み"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSvnHBYPb6TQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "49fe0e39-1bd0-4926-97b9-9e0060018573"
      },
      "source": [
        "filepath = \"DATAPATH/train_data.csv\"\n",
        "df = pd.read_csv(filepath)\n",
        "print(df.shape)\n",
        "df.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(30000, 2)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>empty</td>\n",
              "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>sadness</td>\n",
              "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>sadness</td>\n",
              "      <td>Funeral ceremony...gloomy friday...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>enthusiasm</td>\n",
              "      <td>wants to hang out with friends SOON!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>neutral</td>\n",
              "      <td>@dannycastillo We want to trade with someone w...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    sentiment                                            content\n",
              "0       empty  @tiffanylue i know  i was listenin to bad habi...\n",
              "1     sadness  Layin n bed with a headache  ughhhh...waitin o...\n",
              "2     sadness                Funeral ceremony...gloomy friday...\n",
              "3  enthusiasm               wants to hang out with friends SOON!\n",
              "4     neutral  @dannycastillo We want to trade with someone w..."
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JEI6SH7b6TU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "854da669-c566-411f-93ab-06555f2ff959"
      },
      "source": [
        "df['sentiment'].value_counts()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "worry         7433\n",
              "neutral       6340\n",
              "sadness       4828\n",
              "happiness     2986\n",
              "love          2068\n",
              "surprise      1613\n",
              "hate          1187\n",
              "fun           1088\n",
              "relief        1021\n",
              "empty          659\n",
              "enthusiasm     522\n",
              "boredom        157\n",
              "anger           98\n",
              "Name: sentiment, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHajyKpmb6TY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2896510-8dfc-40f3-e477-5a55af904446"
      },
      "source": [
        "# 3カテゴリだけ残す\n",
        "shortlist = ['neutral', \"happiness\", \"worry\"]\n",
        "df_subset = df[df['sentiment'].isin(shortlist)]\n",
        "df_subset.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16759, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2oiZzU5b6Tf"
      },
      "source": [
        "## テキストの前処理\n",
        "\n",
        "Tweetでは以下のことを考慮する必要があります。\n",
        "\n",
        "- @mentionやURLは除去するか\n",
        "- 通常のトークナイザーではなくTweet用のトークナイザーを使うか\n",
        "- ストップワードや数字はどうするか"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rl-FfMdLb6Th",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06771b9e-7a06-4dca-b6e2-d60077126a6b"
      },
      "source": [
        "# strip_handlesはTwitterのハンドル名のような個人情報を除去します。\n",
        "# そのような情報はTweetの感情を分類するのにあまり貢献しないと考えられます。\n",
        "# preserve_case=Falseは、小文字に変換するための引数です。\n",
        "tweeter = TweetTokenizer(strip_handles=True, preserve_case=False)\n",
        "mystopwords = set(stopwords.words(\"english\"))\n",
        "\n",
        "# Tweetをトークナイズし、ストップワードや数字を除去するための関数です。\n",
        "# 句読点と感情を表す顔文字などは、タスクに関連するので残しておきます。\n",
        "def preprocess_corpus(texts):\n",
        "    def remove_stops_digits(tokens):\n",
        "        # 入れ子になった関数。ストップワードと数字をトークンのリストから除去\n",
        "        return [token for token in tokens if token not in mystopwords and not token.isdigit()]\n",
        "    # 上記で定義した関数を使って、Twitterトークナイザーの出力をさらに処理\n",
        "    return [remove_stops_digits(tweeter.tokenize(content)) for content in texts]\n",
        "\n",
        "# df_subsetは3つのカテゴリのみを含む\n",
        "mydata = preprocess_corpus(df_subset['content'])\n",
        "mycats = df_subset['sentiment']\n",
        "print(len(mydata), len(mycats))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16759 16759\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--_CkQPCB4BQ"
      },
      "source": [
        "## Doc2Vecの学習"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsGwfVebb6Tl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30633d42-7f7b-4e4f-a6eb-af5907a7d7d8"
      },
      "source": [
        "#　データを学習用とテスト用に分割\n",
        "train_data, test_data, train_cats, test_cats = train_test_split(\n",
        "    mydata,\n",
        "    mycats,\n",
        "    random_state=1234\n",
        ")\n",
        "\n",
        "# doc2vec形式の学習データを準備\n",
        "train_doc2vec = [TaggedDocument((d), tags=[str(i)]) for i, d in enumerate(train_data)]\n",
        "\n",
        "# Tweetの表現を学習するために、doc2vecモデルを学習\n",
        "model = Doc2Vec(vector_size=50, alpha=0.025, min_count=5, dm=1, epochs=100)\n",
        "model.build_vocab(train_doc2vec)\n",
        "model.train(train_doc2vec, total_examples=model.corpus_count, epochs=model.epochs)\n",
        "model.save(\"d2v.model\")\n",
        "print(\"Model Saved\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaQ5uPgBB6ZD"
      },
      "source": [
        "## 分類モデルの学習"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTqo26Vsb6Ts",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92556797-40b0-4d10-e017-d68a4bf44b3c"
      },
      "source": [
        "# 学習したモデルを用いて、学習データとテストデータに対する特徴表現を推論\n",
        "model= Doc2Vec.load(\"d2v.model\")\n",
        "\n",
        "# 安定した表現を得るために、複数ステップの推論をする\n",
        "train_vectors =  [model.infer_vector(list_of_tokens, steps=50) for list_of_tokens in train_data]\n",
        "test_vectors = [model.infer_vector(list_of_tokens, steps=50) for list_of_tokens in test_data]\n",
        "\n",
        "# ロジスティック回帰のような分類機を使う\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# クラスは不均衡なので、class_weight=\"balanced\"を設定\n",
        "myclass = LogisticRegression(class_weight=\"balanced\")\n",
        "myclass.fit(train_vectors, train_cats)\n",
        "\n",
        "preds = myclass.predict(test_vectors)\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "print(classification_report(test_cats, preds))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "   happiness       0.33      0.52      0.40       713\n",
            "     neutral       0.46      0.53      0.49      1595\n",
            "       worry       0.60      0.39      0.47      1882\n",
            "\n",
            "    accuracy                           0.47      4190\n",
            "   macro avg       0.46      0.48      0.46      4190\n",
            "weighted avg       0.50      0.47      0.47      4190\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQuoinmiK_jG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}