{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03_Word2Vec_Example.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVtvH58nb_Hp"
      },
      "source": [
        "# Word2Vecを用いたテキスト分類\n",
        "\n",
        "このノートブックでは、事前学習済みWord2vecモデルを使って、特徴抽出とテキスト分類をする方法を紹介します。\n",
        "\n",
        "このデータセットは、Amazon、Yelp、IMDBから肯定的・否定的な文を各1500件ずつ集めて構築しています。\n",
        "データセットとしては、UCIリポジトリのSentiment labelled sentencesを使用します。\n",
        "\n",
        "- http://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences\n",
        "\n",
        "\n",
        "事前学習済みの埋め込みモデルとしては、Google Newsのベクトルを使います。\n",
        "\n",
        "- https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CHrpjVoFYc0"
      },
      "source": [
        "## 準備\n",
        "\n",
        "### インポート"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQX8DAmBb_Hr",
        "outputId": "f163344e-60c3-4169-8791-b0864bf2bb2f"
      },
      "source": [
        "import os\n",
        "from string import punctuation\n",
        "from time import time\n",
        "\n",
        "import nltk\n",
        "import numpy as np\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyqZcnJCGo97"
      },
      "source": [
        "### データセットのダウンロード"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCrmWLmhFqdj",
        "outputId": "73f9ef72-2f89-407c-e8f1-b1bb28d80116"
      },
      "source": [
        "!mkdir DATAPATH\n",
        "!wget -P DATAPATH http://archive.ics.uci.edu/ml/machine-learning-databases/00331/sentiment%20labelled%20sentences.zip\n",
        "!unzip \"DATAPATH/sentiment labelled sentences.zip\""
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'sentiment labelled sentences.zip'  'sentiment labelled sentences.zip.1'\n",
            "Archive:  DATAPATH/sentiment labelled sentences.zip\n",
            "   creating: sentiment labelled sentences/\n",
            "  inflating: sentiment labelled sentences/.DS_Store  \n",
            "   creating: __MACOSX/\n",
            "   creating: __MACOSX/sentiment labelled sentences/\n",
            "  inflating: __MACOSX/sentiment labelled sentences/._.DS_Store  \n",
            "  inflating: sentiment labelled sentences/amazon_cells_labelled.txt  \n",
            "  inflating: sentiment labelled sentences/imdb_labelled.txt  \n",
            "  inflating: __MACOSX/sentiment labelled sentences/._imdb_labelled.txt  \n",
            "  inflating: sentiment labelled sentences/readme.txt  \n",
            "  inflating: __MACOSX/sentiment labelled sentences/._readme.txt  \n",
            "  inflating: sentiment labelled sentences/yelp_labelled.txt  \n",
            "  inflating: __MACOSX/._sentiment labelled sentences  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgiYwhZfGVyB"
      },
      "source": [
        "!mv \"sentiment labelled sentences/\"*.txt .\n",
        "!cat amazon_cells_labelled.txt imdb_labelled.txt yelp_labelled.txt > DATAPATH/sentiment_sentences.txt"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZAVWDFgHiIS"
      },
      "source": [
        "import gdown\n",
        "gdown.download('https://drive.google.com/u/0/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM', 'GoogleNews-vectors-negative300.bin.gz', quiet=False)\n",
        "!gunzip GoogleNews-vectors-negative300.bin.gz\n",
        "!mv GoogleNews-vectors-negative300.bin DATAPATH"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOcp8JdGFdy4"
      },
      "source": [
        "### データの読み込み"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COUGXAxcb_H5",
        "scrolled": true,
        "outputId": "75529e5d-b633-414a-8e48-dabfa0fa1ccb"
      },
      "source": [
        "# 事前学習済みword2vecモデルとデータセットの読み込み\n",
        "data_path= \"DATAPATH\"\n",
        "path_to_model = os.path.join(data_path,'GoogleNews-vectors-negative300.bin')\n",
        "training_data_path = os.path.join(data_path, \"sentiment_sentences.txt\")\n",
        "\n",
        "# Word2vecの読み込み。時間がかかる。\n",
        "%time w2v_model = KeyedVectors.load_word2vec_format(path_to_model, binary=True)\n",
        "print('done loading Word2Vec')\n",
        "\n",
        "# テキストデータとカテゴリの読み込み\n",
        "# ファイルはタブ区切りで、文とカテゴリから構成されている\n",
        "texts = []\n",
        "cats = []\n",
        "fh = open(training_data_path)\n",
        "for line in fh:\n",
        "    text, sentiment = line.split(\"\\t\")\n",
        "    texts.append(text)\n",
        "    cats.append(sentiment)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 44.2 s, sys: 3.69 s, total: 47.9 s\n",
            "Wall time: 1min 17s\n",
            "done loading Word2Vec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-WjFyC6b_IE",
        "outputId": "8d9b7148-5e99-400c-ad3f-e5c865b4e7e2"
      },
      "source": [
        "# モデルの確認\n",
        "word2vec_vocab = w2v_model.vocab.keys()\n",
        "word2vec_vocab_lower = [item.lower() for item in word2vec_vocab]\n",
        "print(len(word2vec_vocab))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XEz30Jztb_IP",
        "outputId": "5491d2cb-8410-44b2-a16a-606ca6c5d370"
      },
      "source": [
        "# データセットの確認\n",
        "print(len(cats), len(texts))\n",
        "print(texts[1])\n",
        "print(cats[1])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3000 3000\n",
            "Good case, Excellent value.\n",
            "1\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inLABBTEKi6S"
      },
      "source": [
        "## テキストの前処理"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFOGaDTwb_Ig",
        "outputId": "1cb30ff9-95b1-4353-8e05-771137aeeeec"
      },
      "source": [
        "# テキストの前処理\n",
        "def preprocess_corpus(texts):\n",
        "    mystopwords = set(stopwords.words(\"english\"))\n",
        "    def remove_stops_digits(tokens):\n",
        "        # 小文字化、ストップワードと数字の除去\n",
        "        return [token.lower() for token in tokens if token not in mystopwords and not token.isdigit()\n",
        "               and token not in punctuation]\n",
        "    return [remove_stops_digits(word_tokenize(text)) for text in texts]\n",
        "\n",
        "texts_processed = preprocess_corpus(texts)\n",
        "print(len(cats), len(texts_processed))\n",
        "print(texts_processed[1])\n",
        "print(cats[1])"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3000 3000\n",
            "['good', 'case', 'excellent', 'value']\n",
            "1\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXRiGtY1b_Iq",
        "outputId": "6d152685-c717-4886-f42a-835add005cc3"
      },
      "source": [
        "# 文に含まれる単語の埋め込みを平均して、特徴ベクトルを作成\n",
        "def embedding_feats(list_of_lists):\n",
        "    DIMENSION = 300\n",
        "    zero_vector = np.zeros(DIMENSION)\n",
        "    feats = []\n",
        "    for tokens in list_of_lists:\n",
        "        feat_for_this =  np.zeros(DIMENSION)\n",
        "        count_for_this = 0\n",
        "        for token in tokens:\n",
        "            if token in w2v_model:\n",
        "                feat_for_this += w2v_model[token]\n",
        "                count_for_this +=1\n",
        "        if count_for_this:\n",
        "          feats.append(feat_for_this / count_for_this)\n",
        "        else:\n",
        "          feats.append(feat_for_this)\n",
        "    return feats\n",
        "\n",
        "\n",
        "train_vectors = embedding_feats(texts_processed)\n",
        "print(len(train_vectors))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44sCbsdcKmeZ"
      },
      "source": [
        "## モデルの学習"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mr9IaQppb_Ix",
        "outputId": "c83d1b3d-dc46-4ac4-f714-f3b69f31f4f7"
      },
      "source": [
        "# モデルの学習\n",
        "classifier = LogisticRegression(random_state=1234)\n",
        "train_data, test_data, train_cats, test_cats = train_test_split(train_vectors, cats)\n",
        "classifier.fit(train_data, train_cats)\n",
        "print(\"Accuracy: \", classifier.score(test_data, test_cats))\n",
        "preds = classifier.predict(test_data)\n",
        "print(classification_report(test_cats, preds))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  0.8386666666666667\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          0\n",
            "       0.84      0.83      0.84       369\n",
            "          1\n",
            "       0.84      0.84      0.84       381\n",
            "\n",
            "    accuracy                           0.84       750\n",
            "   macro avg       0.84      0.84      0.84       750\n",
            "weighted avg       0.84      0.84      0.84       750\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7wjLB8rb_JB"
      },
      "source": [
        "悪くない性能です。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8z3tJJJkb_JB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}